{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c68452-4696-4397-8ced-3c6953af0b2b",
   "metadata": {},
   "source": [
    "# Preparing highRES demand.dd and temopral.dd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7e6b1-49c2-4f2f-89a1-dea2fc8eb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pathlib\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c6afd-9405-4439-a89f-ef72bf87591f",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2982f-4f34-42c6-8efb-94089ee34b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Years = snakemake.wildcards.year  \n",
    "Years_int = int(Years)\n",
    "date_range = snakemake.params.date_range\n",
    "Europe_countries = pd.read_csv(snakemake.input[\"europecountriescsvlocation\"])  # from snakeflow\n",
    "Europe_demand = pd.read_csv(snakemake.input[\"europedemandcsvlocation\"])  # Input from rule build_demand\n",
    "#scenarios = pd.read_excel('gb_ext_scenarios.xls', sheet_name=\"scenario_annual_dem\", skiprows=0)\n",
    "#euro31_dem_2050 = snakemake.params.euro31_dem_2050  # Yearly Twh demand for rescaling, if necessary\n",
    "# How to deal with hardcore rescale?\n",
    "#rescale = snakemake.params.rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8dd40-c5ad-46a2-add6-1f61cdc5891a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Europe_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d383c84-768e-43c1-ba7c-06d7d873beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ebecd-60aa-46b0-828a-585f145fb35a",
   "metadata": {},
   "source": [
    "## Demand.dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76abe9d6-9848-4d9d-ab41-a9de17fe9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathlib.Path(snakemake.output[0]).parent\n",
    "#opath = pathlib.Path('/fp/homes01/u01/ec-javedm/ec85/models/shahzad/private/highRES_demand').parent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb0d18-faa2-40fe-9680-3a531d41c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = [Years + \"-\" + date for date in date_range]\n",
    "dstart = datetime.datetime.fromisoformat(date_range[0])\n",
    "dstop = datetime.datetime.fromisoformat(date_range[1]) + datetime.timedelta(hours=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13d584-b8dc-4b56-9326-9c4e95610dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries selection for modeling coming from snakemake work flow\n",
    "etm_countries = Europe_countries[Europe_countries[\"ETM\"] == 1][\"ISO2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b88bc-645b-4124-ac2c-66bf376e467e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etm_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139f82b-f707-4f4a-b2c7-9903e2341912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'datetime' column to actual datetime objects for filtering\n",
    "Europe_demand['datetime'] = pd.to_datetime(Europe_demand['datetime'])\n",
    "# Set the 'datetime' column as the index\n",
    "Europe_demand = Europe_demand.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b15290-cd00-45aa-8f85-be469e4f3923",
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b3e17-6095-4ee7-b8a2-12ac6fb76504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select demand data for ETM countries only\n",
    "Europe_demand_etm = Europe_demand[etm_countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5976c-f6be-49e8-ba93-cd39e5e3a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by the specified date range\n",
    "Europe_demand_etm = Europe_demand_etm.loc[dstart:dstop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaa5dc-299e-46bf-8d53-2b9fa53d4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_demand_etm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baeef1a-78e2-48cb-a947-29ab8674d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_demand_etm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712de0a-6a6a-4827-908c-7469b619fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any countries with missing data\n",
    "if Europe_demand_etm.shape[1] != len(etm_countries):\n",
    "    print(\"Countries missing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea6d77-c97b-4f06-af1a-a9d422f6ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zero demands with NaN to prepare for interpolation\n",
    "Europe_demand_etm.replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0690ca8-6362-4bf2-bbee-871d0b91050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values\n",
    "Europe_demand_etm.interpolate(limit=2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a23802-de2d-42d5-b795-ce86cbb64687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's still any missing data, and print them if found\n",
    "if Europe_demand_etm.isnull().any().any():\n",
    "    for column in Europe_demand_etm.columns[Europe_demand_etm.isnull().any()]:\n",
    "        print(f\"Missing data found in column: {column}\")\n",
    "        missing_rows = Europe_demand_etm[Europe_demand_etm[column].isnull()]\n",
    "        print(missing_rows)\n",
    "    print(\"Countries with remaining missing demand data:\",\n",
    "          Europe_demand_etm.columns[Europe_demand_etm.isnull().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ceccf5-a7c6-4946-8834-84228d1d9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Years_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c17f1a-d004-439d-862e-2b22412f8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data for leap year's 29th of February\n",
    "#if calendar.isleap(Years_int):\n",
    "#    print(f\"{Years_int} is a leap year\")\n",
    "#    # You may need to adjust the following line if your index isn't in hourly format\n",
    "#    feb_29_index = pd.date_range(f\"{Years_int}-02-29 00:00\", f\"{Years_int}-02-29 23:00\", freq=\"H\")\n",
    "#    feb_29_data = pd.DataFrame(index=feb_29_index, columns=Europe_demand_etm.columns)\n",
    "\n",
    "    # Here replicate the demand from February 28th\n",
    "#    for col in feb_29_data.columns:\n",
    "#        feb_29_data[col] = Europe_demand_etm.loc[f\"{Years_int}-02-28\"].values\n",
    "\n",
    "    # Append the February 29th data\n",
    "#    Europe_demand_etm = Europe_demand_etm.append(feb_29_data).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f1278-1325-4655-9c52-e26b3f66555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the year is a leap year\n",
    "if calendar.isleap(Years_int):\n",
    "    print(f\"{Years_int} is a leap year\")\n",
    "    # Define the date range for February 29th\n",
    "    feb_29_range = pd.date_range(start=f\"{Years_int}-02-29 00:00\", end=f\"{Years_int}-02-29 23:00\", freq='h')\n",
    "    \n",
    "    # Keep only the rows that are not in the February 29 range\n",
    "    Europe_demand_etm = Europe_demand_etm[~Europe_demand_etm.index.isin(feb_29_range)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea4ca8-12c4-45c9-ae13-225ab344666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_demand_etm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdab6f-eb49-4402-9a99-1228f625a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_demand_etm.sum().sum()  # while the value in excel file for 2050 is 5870 TWH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26099e72-10f7-413e-be66-377f98d8e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly demand rescaling with respect to meet annual demand target  \n",
    "#if rescale == \"annual\":\n",
    "#    out_flg = \"annual\"\n",
    "#    if Europe_demand_etm.shape[0] >= 8760.0:\n",
    "#        # Rescale the DataFrame so its sum matches the annual demand\n",
    "#        Europe_demand_etm = Europe_demand_etm * (euro31_dem_2050 * 1e6 / Europe_demand_etm.sum().sum())\n",
    "#else:\n",
    "#    out_flg = \"norescale\"\n",
    "#print(f\"{out_flg} Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b673615-606f-4244-8566-5fcb82d19b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_demand_etm.sum().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d899918-ade7-4d48-9572-d11dcbfa6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2dd(data, sets, all_combin=False, rounddp=8):  #all_combin input could be removed\n",
    "    # Verify if the input is already a NumPy array or convert it to one\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = np.array(data)\n",
    "     \n",
    "    sets = np.array(sets, dtype=\"object\")\n",
    "\n",
    "    # Each set in \"sets\" needs to be 1D at the moment\n",
    "    # \"sets\" and \"data\" must be in correct order -> last set must be column headers, previous sets are rows\n",
    "\n",
    "    # Round the data first\n",
    "    data = np.round(data.astype(float), rounddp)\n",
    "    \n",
    "    # Check the lengths of sets to determine how to combine them\n",
    "    lens = np.array([item.shape[0] for item in sets])\n",
    "    \n",
    "    # If all_combin is True, we want to calculate the cartesian product\n",
    "    # This is for the situation where every possible set combination is needed\n",
    "    if all_combin:\n",
    "        # Generate all combinations of the sets, create a matrix where each row is a combination\n",
    "        sets_out = np.array(list(itertools.product(*sets)))\n",
    "        \n",
    "        # Concatenate the elements in each combination with a period ('.')\n",
    "        sets_new = np.array(['.'.join(map(str, item)) for item in sets_out])\n",
    "        \n",
    "        # Transform 'sets_new' into a column vector, to be combined with 'data'\n",
    "        sets_new = sets_new.reshape(-1, 1)\n",
    "    \n",
    "    # If all_combin is False, we are assuming that the last set corresponds to data columns\n",
    "    # And the remaining sets correspond to data rows (if there are more than one)\n",
    "    else:\n",
    "        if len(sets) == 1:\n",
    "            sets_new = sets[0].astype(str).reshape(-1, 1)\n",
    "        else:\n",
    "            # Repeat or tile sets if necessary\n",
    "            sets_new = [np.tile(s, int(lens.max()/len(s))) if len(s) < lens.max() else s for s in sets]\n",
    "            sets_new = np.array(['.'.join(map(str, comb)) for comb in zip(*sets_new)])\n",
    "            sets_new = sets_new.reshape(-1, 1)\n",
    "\n",
    "    # Combine 'sets_new' with 'data', side by side\n",
    "    data_out = np.hstack((sets_new, data.reshape(-1, 1)))\n",
    "\n",
    "    # Return the formatted output data\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c48a4e-1fb2-453e-9ab0-eaa2bf941f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(Europe_demand_etm.shape[0])   #length of rows\n",
    "z = Europe_demand_etm.columns.values\n",
    "\n",
    "dd_data = data2dd(Europe_demand_etm.values.T, [z, t], all_combin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfa1a4-25d9-43b0-bb15-6798b1bddf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7f089-3341-49f3-b037-2e56a81ee8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.array([[\"parameter\", \"\"], [\"demand\" + \" /\", \"\"]])\n",
    "bottom = np.array([[\"/\", \"\"], [\"\", \"\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11819f31-f82a-4be2-8597-0ac78e03d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_data = np.concatenate((top, dd_data, bottom), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e91cd-48dc-41df-ae8f-25bc7ae76b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3754e6-70e2-4d5b-a917-6b56b2ced726",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile1 = snakemake.output[0]\n",
    "# Format the data as a string before writing\n",
    "dd_data_str = '\\n'.join([' '.join(row) for row in dd_data])\n",
    "with open(outfile1, 'w') as file:\n",
    "    file.write(dd_data_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f911137-28e2-43b0-a85d-b27470e1a412",
   "metadata": {},
   "source": [
    "## Temporal2dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0c711-edc5-4274-98a2-0ca1b533aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943f6b9-e2ee-4b32-bbe4-39570d039467",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a7c0c-59ab-479b-a6b5-022bd17a976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile2 = snakemake.output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a05f1-ef10-4ef9-b2e3-649f56b4416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the range of years and total number of time periods (hours)\n",
    "years = np.arange(dstart.year, dstop.year + 1)\n",
    "ntime = ((dstop - dstart).total_seconds() / 3600) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b34b9d-b4aa-4582-9e59-68695d572351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473857f8-2961-4e5f-ae13-a8b8dcad5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an hour-to-year mapping\n",
    "hr2yr = []\n",
    "for nyr, yr in enumerate(years):\n",
    "    shour = ((datetime.datetime(yr, 1, 1, 0) - dstart).total_seconds() / 3600) + 1\n",
    "    if dstop.year == yr:\n",
    "        ehour = ((dstop - dstart).total_seconds() / 3600) + 1\n",
    "    else:\n",
    "        ehour = (\n",
    "            (datetime.datetime(yr, 12, 31, 23) - dstart).total_seconds() / 3600\n",
    "        ) + 1                    #hardcoded: 12/31/23\n",
    "\n",
    "    hrs = np.arange(shour - 1, ehour).astype(int)\n",
    "    hr2yr.append(list(zip(np.repeat(nyr, hrs.shape[0]).astype(int), hrs)))\n",
    "\n",
    "hr2yr = np.char.array(np.vstack(hr2yr).astype(str))\n",
    "\n",
    "hr2yr = (hr2yr[:, 0] + \".\" + hr2yr[:, 1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f46dd3-e77d-4715-ba84-ae740e22a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr2yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76fa6b-9af4-4676-9470-2b40d878a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the year or year range as a string for the output\n",
    "if dstart.year != dstop.year:\n",
    "    yr = str(dstart.year) + \"-\" + str(dstop.year)\n",
    "else:\n",
    "    yr = str(dstart.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c865a9-95c9-4743-b64a-4879b07ad1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820dbe1-1894-4f0c-ba4c-0e480638e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the \"hourly\" set\n",
    "h_set = np.arange(ntime).reshape(-1, 1).astype(int)\n",
    "top_h = np.array([[\"set\"], [\"h /\"]])\n",
    "bottom_h = np.array([[\"/\"], [\"\"]])\n",
    "h_set_data = np.concatenate((top_h, h_set, bottom_h), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799ceda-ef07-461d-a6b4-10c993e9993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8257390-4e24-45b1-9448-4f97bfdc2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the \"yr\" set\n",
    "yr_set = np.arange(years.shape[0]).reshape(-1, 1).astype(int)\n",
    "top_yr = np.array([[\"set\"], [\"yr /\"]])\n",
    "bottom_yr = np.array([[\"/\"], [\"\"]])\n",
    "yr_set_data = np.concatenate((top_yr, yr_set, bottom_yr), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94843e-02dc-4f8a-b10c-c3b7faa1e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560a2b5-b64a-4fd2-9690-9ad7564a88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the \"hr2yr_map\" set\n",
    "top_hr2yr = np.array([[\"set\"], [\"hr2yr_map /\"]])\n",
    "bottom_hr2yr = np.array([[\"/\"], [\"\"]])\n",
    "hr2yr_map_data = np.concatenate((top_hr2yr, hr2yr, bottom_hr2yr), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f9110-f01e-4853-8e7c-fa8442d4f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr2yr_map_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcd765-a721-4d41-abe3-5d35eaf142f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all formatted sets\n",
    "concatenate_sets = np.concatenate((h_set_data, yr_set_data, hr2yr_map_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75021e-740d-4beb-8e65-4fe3033a5133",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd7d91-1ff9-416e-96b8-559a2f08f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy array to a string for writing to file\n",
    "concatenate_sets_str = '\\n'.join(' '.join(row) for row in concatenate_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c0f28-3cf8-40ec-bc19-453b706c924c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(outfile2, 'w') as file:\n",
    "    file.write(concatenate_sets_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167697a-82c8-4271-b293-a47a38f4af85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
